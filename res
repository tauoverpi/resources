#topic:actor
#medium:paper
Actor Model of Computation: Scalable Robust Information Systems
The Actor Model is a mathematical theory that treats “Actors” as the
universal primitives of digital computation.  The model has been used
both as a framework for a theoretical understanding of concurrency, and as
the theoretical basis for several practical implementations of concurrent
systems. The advent of massive concurrency through client-cloud computing
and many-core computer architectures has galvanized interest in the Actor Model
^
@https://arxiv.org/pdf/1008.1459.pdf
#topic:actor
#medium:paper
Formalizing common sense reasoning for scalable inconsistency-robust information integration using Direct LogicTM Reasoning and the Actor Model
People use common sense in their interactions with large information systems.
This common sense needs to be formalized so that it can be used by computer
systems. Unfortunately, previous formalizations have been inadequate. For
example, classical logic is not safe for use with pervasively inconsistent
information. The goal is to develop a standard foundation for reasoning in
large-scale Internet applications (including sense making for natural
language)
^
@https://arxiv.org/pdf/0812.4852.pdf
#topic:bigraphs
#topic:category theory
#medium:paper
Bigraphical reactive systems: basic theory
A notion of bigraph is proposed as the basis for a model of mobile
interaction. A bigraph consists of two independent structures: a topograph
representing locality and a monograph representing connectivity. Bigraphs
are equipped with reaction rules to form bigraphical reactive systems (BRSs),
which include versions of the -calculus and the ambient calculus. Bigraphs are
shown to be a special case of a more abstract notion, wide reactive systems
(WRSs), not assuming any particular graphical or other structure but equipped
with a notion of width, which expresses that agents, contexts and reactions
may all be widely distributed entities
^
@https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-523.pdf
#topic:mbus
#topic:wireless mbus
#topic:security
#medium:paper
Wireless M-Bus Security Whitepaper
This work aims to analyse the security of the Meter Bus as specified
in the relevant International organisation for Standardization (ISO)
documentation. M-Bus has its roots in the heat metering industries and
was continuously adopted to fit more complex applications. M-Bus is the
communication bus of choice of several meter manufacturers and its applications
span from drive-by wireless meter reading over to meter-to-meter and mesh
networking to meter-to-collector communication. M-Bus implementations support
different media types such as power line carrier (PLC) or twisted-pair
bus. To avoid the wiring efforts at the distribution level, utilities,
metering companies and manufacturers tend to more frequently choose wireless
protocols for communication. Accordingly, the analysis will mainly concentrate
on M-Bus wireless based communication – wM-Bus
^
@https://www.compass-security.com/fileadmin/Datein/Research/Praesentationen/blackhat_2013_wmbus_security_whitepaper.pdf
#topic:fpga
#topic:neural network
#topic:machine learning
#medium:paper
FPGA Implementations of Neural Networks
During the 1980s and early 1990s there was significant work in the design
and implementation of hardware neurocomputers. Nevertheless, most of
these efforts may be judged to have been unsuccessful: at no time have
have hardware neurocomputers been in wide use. This lack of success may be
largely attributed to the fact that earlier work was almost entirely aimed
at developing custom neurocomputers, based on ASIC technology, but for such
niche areas this technology was never sufficiently developed or competitive
enough to justify large-scale adoption. On the other hand, gate-arrays of
the period mentioned were never large enough nor fast enough for serious
artificial-neuralnetwork (ANN) applications. But technology has now improved:
the capacity and performance of current FPGAs are such that they present
a much more realistic alternative. Consequently neurocomputers based on
FPGAs are now a much more practical proposition than they have been in the
past. This book summarizes some work towards this goal and consists of 12
papers that were selected, after review, from a number of submissions.
^
@http://lab.fs.uni-lj.si/lasin/wp/IMIT_files/neural/doc/Omondi2006.pdf
#topic:fpga
#topic:neural network
#topic:machine learning
#medium:paper
A General Neural Network Hardware Architecture on FPGA
Field Programmable Gate Arrays (FPGAs) plays an increasingly important
role in data sampling and processing industries due to its highly
parallel architecture, low power consumption, and flexibility in custom
algorithms. Especially, in the artificial intelligence field, for training and
implement the neural networks and machine learning algorithms, high energy
efficiency hardware implement and massively parallel computing capacity are
heavily demanded. Therefore, many global companies have applied FPGAs into
AI and Machine learning fields such as autonomous driving and Automatic
Spoken Language Recognition (Baidu) [1] [2] and Bing search (Microsoft)
[3]. Considering the FPGAs great potential in these fields, we tend to
implement a general neural network hardware architecture on XILINX ZU9CG
System On Chip (SOC) platform [4], which contains abundant hardware resource
and powerful processing capacity. The general neural network architecture
on the FPGA SOC platform can perform forward and backward algorithms in deep
neural networks (DNN) with high performance and easily be adjusted according
to the type and scale of the neural networks.
^
@https://arxiv.org/ftp/arxiv/papers/1711/1711.05860.pdf
#topic:file system
#medium:paper
TagFS: A simple tag-based filesystem
TagFS is a simple yet effective tag-based filesystem. Instead of organizing files and documents in
a strict hierarchy (like traditional filesystems), TagFS allows users to assign descriptive attributes
(called tags) to files and subsequently locate those files by searching for tags of interest.
^
@https://web.mit.edu/6.033/2011/wwwdocs/writing-samples/sbezek_dp1.pdf
#topic:optimisation
#topic:neural network
#topic:genetic algorithms
#topic:differential evolution
#topic:bee swarm algorithm
#topic:ant colony optimisation
#medium:book
Clever Algorithms: Nature-Inspired Programming Recipes
Implementing an Artificial Intelligence algorithm is difficult. Algorithm
descriptions may be incomplete, inconsistent, and distributed across a
number of papers, chapters and even websites. This can result in varied
interpretations of algorithms, undue attrition of algorithms, and ultimately
bad science. This book is an effort to address these issues by providing
a handbook of algorithmic recipes drawn from the fields of Metaheuristics,
Biologically Inspired Computation and Computational Intelligence, described in
a complete, consistent, and centralized manner. These standardized descriptions
were carefully designed to be accessible, usable, and understandable. Most
of the algorithms described were originally inspired by biological and
natural systems, such as the adaptive capabilities of genetic evolution
and the acquired immune system, and the foraging behaviors of birds, bees,
ants and bacteria. An encyclopedic algorithm reference, this book is intended
for research scientists, engineers, students, and interested amateurs. Each
algorithm description provides a working code example in the Ruby Programming
Language.
^
@https://raw.githubusercontent.com/clever-algorithms/CleverAlgorithms/master/release/clever_algorithms.pdf
#topic:category theory
#topic:haskell
#topic:c++
#topic:template metaprogramming
#topic:type system
#author:Bartosz Milewski
Category Theory for Programmers
For some time now I’ve been floating the idea of writing a book about
category theory that would be targeted at programmers. Mind you, not computer
scientists but programmers — engineers rather than scientists. I know
this sounds crazy and I am properly scared. I can’t deny that there is
a huge gap between science and engineering because I have worked on both
sides of the divide. But I’ve always felt a very strong compulsion to
explain things. I have tremendous admiration for Richard Feynman who was
the master of simple explanations. I know I’m no Feynman, but I will try
my best. I’m starting by publishing this preface — which is supposed
to motivate the reader to learn category theory — in hopes of starting a
discussion and soliciting feedback.
^
@https://github.com/hmemcpy/milewski-ctfp-pdf/releases/download/v1.3.0/category-theory-for-programmers.pdf
#topic:category theory
#topic:version control
#author:Samuel Mimram
#author:Cinzia Di Giusto
#medium:paper
A Categorical Theory of Patches
When working with distant collaborators on the same documents, one often
uses a version control system, which is a program tracking the history of
files and helping importing modifications brought by others as patches. The
implementation of such a system requires to handle lots of situations depending
on the operations performed by users on files, and it is thus difficult to
ensure that all the corner cases have been correctly addressed. Here, instead
of verifying the implementation of such a system, we adopt a complementary
approach: we introduce a theoretical model, which is defined abstractly
by the universal property that it should satisfy, and work out a concrete
description of it. We begin by defining a category of files and patches,
where the operation of merging the effect of two coinitial patches is defined
by pushout. Since two patches can be incompatible, such a pushout does not
necessarily exist in the category, which raises the question of which is the
correct category to represent and manipulate files in conflicting state. We
provide an answer by investigating the free completion of the category of files
under finite colimits, and give an explicit description of this category:
its objects are finite sets labeled by lines equipped with a transitive
relation and morphisms are partial functions respecting labeling and relations.
^
@https://arxiv.org/pdf/1311.3903
#topic:algorithm
#author:Eugene W. Myers
#medium:paper
An O(ND) Difference Algorithm and Its Variations∗
The problems of finding a longest common subsequence of two sequences A
and B and a shortest edit script for transforming A into B have long been
known to be dual problems. In this paper, they are shown to be equivalent to
finding a shortest/longest path in an edit graph. Using this perspective,
a simple O(ND) time and space algorithm is developed where N is the sum of
the lengths of A and B and D is the size of the minimum edit script for A
and B. The algorithm performs well when differences are small (sequences are
similar) and is consequently fast in typical applications. The algorithm is
shown to have O(N + D^2) expected-time performance under a basic stochastic
model. A refinement of the algorithm requires only O(N) space, and the use
of suffix trees leads to an O(NlgN + D^2) time variation
^
@http://www.xmailserver.org/diff2.pdf
#topic:sorting
#author:Sergei Bespamyatnikh
#author:Michael Segal
#medium:paper
Enumerating Longest Increasing Subsequences and Patience Sorting (2000)
In this paper we present three algorithms that solve three combinatorial
optimization problems related to each other. One of them is the patience
sorting game, invented as a practical method of sorting real decks of
cards. The second problem is computing the longest monotone increasing
subsequence of the given sequence of n positive integers in the range 1; : :
: ; n. The third problem is to enumerate all the longest monotone increasing
subsequences of the given permutation.
^
@http://www.ii.uni.wroc.pl/~lorys/IPL/article76-1-1.pdf
#topic:single static assignment
#topic:compiler
#topic:compiler optimisation
#author:Lori Carter
#author:Beth Simon
#author:Brad Calder
#author:Larry Carter
#author:Jeanne Ferrante
Predicated Static Single Assignment
Increases in instruction level parallelism are needed to exploit the potential
parallelism available in future wide issue architectures. Predicated execution
is an architectural mechanism that increases instruction level parallelism
by removing branches and allowing simultaneous execution of multiple paths
of control, only committing instructions from the correct path. In order
for the compiler to expose such parallelism, traditional compiler data-flow
analysis needs to be extended to predicated code.  In this paper, we present
Predicated Static Single Assignment (PSSA) to enable aggressive predicated
optimization and instruction scheduling. PSSA removes false dependences by
exploiting renaming and information about the multiple control paths. We
demonstrate the usefulness of PSSA for Predicated Speculation and Control
Height Reduction. These two predicated code optimizations used during
instruction scheduling reduce the dependence length of the critical paths
through a predicated region. Our results show that using PSSA to enable
speculation and control height reduction reduces execution time from 10%
to 58%.
^
@http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.18.7756&rep=rep1&type=pdf
#topic:single static assignment
#topic:compiler
#topic:compiler optimisation
Static Single Assignment Book

^
@http://ssabook.gforge.inria.fr/latest/book.pdf
#topic:neural network
#topic:auto-encoding
#author:Diederik P. Kingma
#author:Max Welling
Auto-Encoding Variational Bayes
How can we perform efficient inference and learning in directed probabilistic
models, in the presence of continuous latent variables with intractable
posterior distributions, and large datasets? We introduce a stochastic
variational inference and learning algorithm that scales to large datasets
and, under some mild differentiability conditions, even works in the
intractable case. Our contributions is two-fold. First, we show that a
reparameterization of the variational lower bound yields a lower bound
estimator that can be straightforwardly optimized using standard stochastic
gradient methods. Second, we show that for i.i.d. datasets with continuous
latent variables per datapoint, posterior inference can be made especially
efficient by fitting an approximate inference model (also called a recognition
model) to the intractable posterior using the proposed lower bound estimator.
Theoretical advantages are reflected in experimental results.
^
@https://arxiv.org/pdf/1312.6114.pdf
#author:Thomas N. Kipf
#author:Max Welling
Variational Graph Auto-Encoders
We introduce the variational graph auto-encoder (VGAE), a framework for
unsupervised learning on graph-structured data based on the variational
auto-encoder (VAE). This model makes use of latent variables and is capable
of learning interpretable latent representations for undirected graphs. We
demonstrate this model using a graph convolutional network (GCN) encoder and
a simple inner product decoder. Our model achieves competitive results on a
link prediction task in citation networks. In contrast to most existing models
for unsupervised learning on graph-structured data and link prediction, our
model can naturally incorporate node features, which significantly improves
predictive performance on a number of benchmark datasets.
^
@https://arxiv.org/pdf/1611.07308.pdf
#author:Carl Doersch
Tutorial on Variational Autoencoders
In just three years, Variational Autoencoders (VAEs) have emerged as one
of the most popular approaches to unsupervised learning of complicated
distributions. VAEs are appealing because they are built on top of standard
function approximators (neural networks), and can be trained with stochastic
gradient descent. VAEs have already shown promise in generating many kinds
of complicated data, including handwritten digits, faces, house numbers,
CIFAR images, physical models of scenes, segmentation, and predicting the
future from static images. This tutorial introduces the intuitions behind
VAEs, explains the mathematics behind them, and describes some empirical
behavior. No prior knowledge of variational Bayesian methods is assumed.
^
@https://arxiv.org/pdf/1606.05908.pdf
#author:Ronald Yu
A Tutorial on VAEs: From Bayes' Rule to Lossless Compression
The Variational Auto-Encoder (VAE) is a simple, efficient, and popular
deep maximum likelihood model. Though usage of VAEs is widespread, the
derivation of the VAE is not as widely understood. In this tutorial, we will
provide an overview of the VAE and a tour through various derivations and
interpretations of the VAE objective. From a probabilistic standpoint, we will
examine the VAE through the lens of Bayes' Rule, importance sampling, and the
change-of-variables formula. From an information theoretic standpoint, we will
examine the VAE through the lens of lossless compression and transmission
through a noisy channel. We will then identify two common misconceptions
over the VAE formulation and their practical consequences. Finally, we will
visualize the capabilities and limitations of VAEs using a code example
(with an accompanying Jupyter notebook) on toy 2D data.
^
@https://arxiv.org/pdf/2006.10273.pdf
